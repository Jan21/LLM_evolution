graph_generation:
  sphere_mesh:
    num_horizontal: 20
    num_vertical: 20
  output:
    file_path: "temp/sphere_mesh_graph.pkl"
    visualize: true
    print_stats: true

data_generation:
  train:
    num_paths: 10000
    min_length: 3
    max_length: 15
  test:
    num_paths: 2000
  graph_path: ${graph_generation.output.file_path}
  output_dir: "."


model:
  d_model: 64
  num_heads: 4
  num_layers: 6
  d_ff: 94
  max_seq_length: 128
  dropout: 0.1
  vocab_size: null  # Will be computed as num_horizontal * num_vertical

training:
  learning_rate: 1e-3
  batch_size: 256
  max_epochs: 3
  weight_decay: 1e-5
  warmup_steps: 1000
  gradient_clip_val: 1.0

data:
  train_file: "train.json"
  test_file: "test.json"
  max_path_length: 64
  num_workers: 4

logging:
  project_name: "path_prediction"
  experiment_name: "transformer_baseline"
  log_every_n_steps: 50

paths:
  data_dir: "./temp"
  model_dir: "./model"
  checkpoint_dir: "./checkpoints"

# Hydra configuration
defaults:
  - _self_
  - override hydra/launcher: ray
  - override hydra/sweeper: basic

# Simple variables - just set these:
num_concurrent_runs: 9                 # X = how many jobs you want running simultaneously  
num_gpus: 2                           # Y = GPUs you're giving to Ray (CUDA_VISIBLE_DEVICES count)
vram_alloc_per_job: 3e9     
cpus_per_job: 2                       # CPUs each job needs       
# Optuna sweeper configuration
hydra:
  launcher:
    ray:
      init:
        num_cpus: ${eval:"${num_concurrent_runs} * ${cpus_per_job}"}     # X * 2 CPUs per job
        num_gpus: ${num_gpus}                                 # Y total GPUs
      remote:
        num_cpus: ${cpus_per_job}                      
        num_gpus: ${eval:"${num_gpus} / ${num_concurrent_runs}"}  # Y/X = GPU per job
        memory: ${vram_alloc_per_job}                  

  sweeper:
    params:
      model.d_model: 64, 128, 256
      model.num_heads: 4, 8, 16
      training.learning_rate: 1e-3, 5e-2, 2e-3